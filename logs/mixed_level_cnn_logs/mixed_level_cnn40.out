{'word_context': 31, 'word_embed_size': 200, 'word_conv_units': 10, 'hidden_units': 1000, 'dropout': False, 'batch_size': 500}
2482
train set done
test set done
[  1,     1] loss: 0.69739
[  1,   501] loss: 0.70203
[  1,  1001] loss: 2.10699
[  1,  1501] loss: 2.92587
[  1,  2001] loss: 0.72164
[  1,  2501] loss: 0.72360
[  1,  3001] loss: 0.74875
[  1,  3501] loss: 0.74306
[  1,  4001] loss: 0.71454
[  1,  4501] loss: 0.70986
[  1,  5001] loss: 0.69906
[  1,  5501] loss: 0.67641
[  1,  6001] loss: 0.70944
[  1,  6501] loss: 0.70936
[  1,  7001] loss: 0.69227
[  1,  7501] loss: 0.67023
[  1,  8001] loss: 0.68126
[  1,  8501] loss: 0.67927
[  1,  9001] loss: 0.65754
[  1,  9501] loss: 0.68788
[  1, 10001] loss: 0.68140
[  1, 10501] loss: 0.66902
[  1, 11001] loss: 0.66832
[  1, 11501] loss: 0.69347
[  1, 12001] loss: 0.71121
[  1, 12501] loss: 0.75822
[  1, 13001] loss: 0.79799
[  1, 13501] loss: 0.79858
[  1, 14001] loss: 0.75961
[  1, 14501] loss: 0.75662
[  1, 15001] loss: 0.76214
[  1, 15501] loss: 0.75524
[  1, 16001] loss: 0.73177
[  1, 16501] loss: 0.71041
[  1, 17001] loss: 0.72511
[  1, 17501] loss: 0.71989
[  1, 18001] loss: 0.75630
[  1, 18501] loss: 0.71170
[  1, 19001] loss: 0.67167
[  1, 19501] loss: 0.69262
[  1, 20001] loss: 0.69491
[  1, 20501] loss: 0.71936
[  1, 21001] loss: 0.71204
[  1, 21501] loss: 0.64595
[  1, 22001] loss: 0.68260
[  1, 22501] loss: 0.64163
[  1, 23001] loss: 0.68895
[  1, 23501] loss: 0.69178
[  1, 24001] loss: 0.70105
[  1, 24501] loss: 0.64591
[  2,     1] loss: 0.67821
[  2,   501] loss: 0.66818
[  2,  1001] loss: 0.70567
[  2,  1501] loss: 0.69787
[  2,  2001] loss: 0.64634
[  2,  2501] loss: 0.71297
[  2,  3001] loss: 0.74341
[  2,  3501] loss: 0.73213
[  2,  4001] loss: 0.69293
[  2,  4501] loss: 0.71031
[  2,  5001] loss: 0.70216
[  2,  5501] loss: 0.67775
[  2,  6001] loss: 0.67045
[  2,  6501] loss: 0.68048
[  2,  7001] loss: 0.63810
[  2,  7501] loss: 0.63044
[  2,  8001] loss: 0.62469
[  2,  8501] loss: 0.60560
[  2,  9001] loss: 0.58116
[  2,  9501] loss: 0.60703
[  2, 10001] loss: 0.61779
[  2, 10501] loss: 0.59457
[  2, 11001] loss: 0.58856
[  2, 11501] loss: 0.60657
[  2, 12001] loss: 0.57250
[  2, 12501] loss: 0.61767
[  2, 13001] loss: 0.62179
[  2, 13501] loss: 0.62929
[  2, 14001] loss: 0.66221
[  2, 14501] loss: 0.74275
[  2, 15001] loss: 0.73464
[  2, 15501] loss: 0.69061
[  2, 16001] loss: 0.72708
[  2, 16501] loss: 0.73158
[  2, 17001] loss: 0.76805
[  2, 17501] loss: 0.70967
[  2, 18001] loss: 0.63081
[  2, 18501] loss: 0.63014
[  2, 19001] loss: 0.63284
[  2, 19501] loss: 0.64421
[  2, 20001] loss: 0.64518
[  2, 20501] loss: 0.64508
[  2, 21001] loss: 0.65775
[  2, 21501] loss: 0.65093
[  2, 22001] loss: 0.67610
[  2, 22501] loss: 0.63877
[  2, 23001] loss: 0.65110
[  2, 23501] loss: 0.61723
[  2, 24001] loss: 0.59779
[  2, 24501] loss: 0.63157
Finished training
Accuracy on train: 0.65300
Accuracy on test: 0.56290
