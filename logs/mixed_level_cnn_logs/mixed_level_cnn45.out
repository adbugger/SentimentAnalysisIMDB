{'word_context': 31, 'word_embed_size': 200, 'word_conv_units': 10, 'hidden_units': 5, 'dropout': True, 'batch_size': 500}
2482
train set done
test set done
[  1,     1] loss: 0.86910
[  1,   501] loss: 1.22979
[  1,  1001] loss: 1.05686
[  1,  1501] loss: 0.78162
[  1,  2001] loss: 0.77231
[  1,  2501] loss: 0.75378
[  1,  3001] loss: 0.74898
[  1,  3501] loss: 0.77286
[  1,  4001] loss: 0.73380
[  1,  4501] loss: 0.74257
[  1,  5001] loss: 0.74983
[  1,  5501] loss: 0.97072
[  1,  6001] loss: 0.75061
[  1,  6501] loss: 0.74213
[  1,  7001] loss: 0.73131
[  1,  7501] loss: 0.79733
[  1,  8001] loss: 0.74524
[  1,  8501] loss: 0.72401
[  1,  9001] loss: 0.82365
[  1,  9501] loss: 0.77527
[  1, 10001] loss: 0.74748
[  1, 10501] loss: 0.75790
[  1, 11001] loss: 0.72296
[  1, 11501] loss: 0.71921
[  1, 12001] loss: 0.74339
[  1, 12501] loss: 0.71298
[  1, 13001] loss: 0.73723
[  1, 13501] loss: 0.72559
[  1, 14001] loss: 0.70267
[  1, 14501] loss: 0.70663
[  1, 15001] loss: 0.71975
[  1, 15501] loss: 0.71613
[  1, 16001] loss: 0.69417
[  1, 16501] loss: 0.71807
[  1, 17001] loss: 0.70886
[  1, 17501] loss: 0.72561
[  1, 18001] loss: 0.70166
[  1, 18501] loss: 0.72468
[  1, 19001] loss: 0.71528
[  1, 19501] loss: 0.71253
[  1, 20001] loss: 0.70454
[  1, 20501] loss: 0.68886
[  1, 21001] loss: 0.70649
[  1, 21501] loss: 0.70376
[  1, 22001] loss: 0.70923
[  1, 22501] loss: 0.71534
[  1, 23001] loss: 0.69351
[  1, 23501] loss: 0.68453
[  1, 24001] loss: 0.82969
[  1, 24501] loss: 0.69908
[  2,     1] loss: 0.70558
[  2,   501] loss: 0.69220
[  2,  1001] loss: 0.68597
[  2,  1501] loss: 0.70756
[  2,  2001] loss: 0.69117
[  2,  2501] loss: 0.69466
[  2,  3001] loss: 0.67736
[  2,  3501] loss: 0.66310
[  2,  4001] loss: 0.68142
[  2,  4501] loss: 0.69072
[  2,  5001] loss: 0.67968
[  2,  5501] loss: 0.68585
[  2,  6001] loss: 0.67123
[  2,  6501] loss: 0.66632
[  2,  7001] loss: 0.69628
[  2,  7501] loss: 0.66951
[  2,  8001] loss: 0.67700
[  2,  8501] loss: 0.67471
[  2,  9001] loss: 0.67598
[  2,  9501] loss: 0.68194
[  2, 10001] loss: 0.70094
[  2, 10501] loss: 0.67027
[  2, 11001] loss: 0.68327
[  2, 11501] loss: 0.68790
[  2, 12001] loss: 0.68331
[  2, 12501] loss: 0.67384
[  2, 13001] loss: 0.67829
[  2, 13501] loss: 0.67496
[  2, 14001] loss: 0.66801
[  2, 14501] loss: 0.66236
[  2, 15001] loss: 0.67268
[  2, 15501] loss: 0.71496
[  2, 16001] loss: 0.67838
[  2, 16501] loss: 0.67806
[  2, 17001] loss: 0.67202
[  2, 17501] loss: 0.68857
[  2, 18001] loss: 0.67792
[  2, 18501] loss: 0.65942
[  2, 19001] loss: 0.66000
[  2, 19501] loss: 0.67233
[  2, 20001] loss: 0.65974
[  2, 20501] loss: 0.65679
[  2, 21001] loss: 0.64770
[  2, 21501] loss: 0.67156
[  2, 22001] loss: 0.67547
[  2, 22501] loss: 0.69153
[  2, 23001] loss: 0.67025
[  2, 23501] loss: 0.67078
[  2, 24001] loss: 0.65700
[  2, 24501] loss: 0.66158
Finished training
Accuracy on train: 0.64300
Accuracy on test: 0.55600
